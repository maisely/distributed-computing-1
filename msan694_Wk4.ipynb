{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "sc = SparkContext.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quiz: Topics between week 3-4. More difficult than quiz 2 and less straightforward. Do homework + practice example. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pair RDDs Operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Actions (on 2 pairs RDDs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **`subtrackByKey(otherDatset)`**: Remove elements\twith a key present in the\tother RDD.\n",
    "    ```python \n",
    "    A = [(2,3), (1,2), (1,3)]\n",
    "    B = [(1,2)]\n",
    "    A.subtract(B) = [(2,3), (1,3)]\n",
    "    A.subtractByKey(B) = [(2,3)]\n",
    "    ``` "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **`join(otherDataset)`** - Perform an inner join between two RDDs. Return in format `(key, (value1, value2))`\n",
    "- **`leftOuterJoin(otherDataset)`** - Perform a join between two RDDs where the key must be present in the **first** RDD.\n",
    "- **`rightOuterJoin(otherDataset)`** - Perform a join between two RDDs where the key must be present in the **second** RDD."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **`cogroup(otherDataset)`**: groupByKey() --> outerjoin()\n",
    " + output format is: (key, (Result Iterable, Result Iterable)\n",
    " + Group data from both RDDs sharing the same key.\n",
    " + Go over two RDDs sharing the same key.\n",
    " + Return the key and the respective lists from two RDD values.\n",
    " + Pairs of (Key, (Resulterable, ResultIteratable)).\n",
    " + Can work on more than two RDDs at once\n",
    " \n",
    " ```python\n",
    " A = [(1,2), (1,2)]\n",
    " B = [(2,2), (2,3), (1,3)]\n",
    " \n",
    " # groupByKey\n",
    " (1, [2,3])\n",
    " (2, [2,3]), (1, [3])\n",
    " \n",
    " # outerjoin\n",
    " (1, ([2,3] , [3]) # the 1st element contains value from rdd1 and the 2nd contains value from rdd2\n",
    " (2, ([], [2,3]) # the 1st element contains value from rdd1 and the 2nd contains value from rdd2\n",
    " \n",
    " #mapValues(lambda x: (list(x[0]), list(x[1]))\n",
    " ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load and split data\n",
    "biz_raw = sc.textFile('/Users/ThyKhueLy/msan694/data/filtered_registered_business_sf.csv')\n",
    "biz = biz_raw.flatMap(lambda l: l.split(\"\\n\")).distinct().map(lambda x: x.split(\",\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load and split data\n",
    "sup_raw = sc.textFile('/Users/ThyKhueLy/msan694/data/supervisor_sf.csv')\n",
    "sup = sup_raw.flatMap(lambda l: l.split(\"\\n\")).distinct().map(lambda x: x.split(\",\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# map key and values\n",
    "biz = biz.map(lambda x: (x[0], x[1]))\n",
    "sup = sup.map(lambda x: (x[0], x[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "no_sup = biz.subtractByKey(sup).values().distinct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39422"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "no_sup.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'Precision Communication Serv',\n",
       " u'Schefer Thomas R',\n",
       " u'Lucid Systems',\n",
       " u'Jacob Abraham',\n",
       " u'Daniel Dela Rosa',\n",
       " u'Sudhir Marahatta',\n",
       " u'Batista Luis S',\n",
       " u'\"Wti',\n",
       " u'Boutin Jacqueline M',\n",
       " u'Avinesh P Singh']"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "no_sup.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "first_num_pairs = sc.parallelize({(2,3),(1,2),(1,3),(2,4),(3,6)})\n",
    "second_num_pairs = sc.parallelize({(1,3),(2,2),(4,6)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "print first_num_pairs.count()\n",
    "print second_num_pairs.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int, {1: 2, 2: 2, 3: 1})"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_num_pairs.countByKey()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(1, 2)], [(1, 3)], [(2, 3)], [(2, 4), (3, 6)]]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_num_pairs.glom().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[], [(1, 3)], [(4, 6)], [(2, 2)]]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "second_num_pairs.glom().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 2), (2, 4), (2, 3), (3, 6)]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_num_pairs.subtract(second_num_pairs).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(3, 6)]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_num_pairs.subtractByKey(second_num_pairs).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(4, 6)]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "second_num_pairs.subtractByKey(first_num_pairs).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, (2, 3)), (1, (3, 3)), (2, (3, 2)), (2, (4, 2))]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_num_pairs.join(second_num_pairs).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int, {1: 2, 2: 2})"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_num_pairs.join(second_num_pairs).countByKey()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, (2, 3)), (1, (3, 3)), (2, (3, 2)), (2, (4, 2)), (3, (6, None))]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_num_pairs.leftOuterJoin(second_num_pairs).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, (2, 3)), (1, (3, 3)), (2, (3, 2)), (2, (4, 2)), (4, (None, 6))]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_num_pairs.rightOuterJoin(second_num_pairs).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int, {1: 2, 2: 2, 3: 1})"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_num_pairs.leftOuterJoin(second_num_pairs).countByKey()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, [2, 3], [3]), (2, [3, 4], [2]), (3, [6], []), (4, [], [6])]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_num_pairs.cogroup(second_num_pairs).map(lambda x: (x[0], list(x[1][0]), list(x[1][1]))).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 3\n",
    "\n",
    "Using Example 1, list (zip,(business_name,supervisor_id)) pairs ordered by\n",
    "supervisor_id.\n",
    "- Only if both business and supervisor exist.\n",
    "- If a business exists.\n",
    "- If a supervisor exists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'94112', (u'Vos Barbara E', u'9')),\n",
       " (u'94112', (u'W J Britton & Co', u'9')),\n",
       " (u'94112', (u'Fernandez Ervin A', u'9')),\n",
       " (u'94112', (u'\"Legaspi', u'9')),\n",
       " (u'94112', (u'Murphy Doris C', u'9')),\n",
       " (u'94112', (u'Daley Margaret T', u'9')),\n",
       " (u'94112', (u'Deng Bisheng', u'9')),\n",
       " (u'94112', (u'Liu Meizhen', u'9')),\n",
       " (u'94112', (u'Tan Shu Kun', u'9')),\n",
       " (u'94112', (u'Rasmussen Maureen Woelfel Katy', u'9'))]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# only if both business and supervisor exist\n",
    "biz_sup0 = biz.join(sup)\n",
    "biz_sup0.sortBy(lambda x: x[1][1], ascending=False).collect()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'94112', (u'Vos Barbara E', u'9')),\n",
       " (u'94112', (u'W J Britton & Co', u'9')),\n",
       " (u'94112', (u'Fernandez Ervin A', u'9')),\n",
       " (u'94112', (u'\"Legaspi', u'9')),\n",
       " (u'94112', (u'Murphy Doris C', u'9')),\n",
       " (u'94112', (u'Daley Margaret T', u'9')),\n",
       " (u'94112', (u'Deng Bisheng', u'9')),\n",
       " (u'94112', (u'Liu Meizhen', u'9')),\n",
       " (u'94112', (u'Tan Shu Kun', u'9')),\n",
       " (u'94112', (u'Rasmussen Maureen Woelfel Katy', u'9'))]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# only if business exists\n",
    "biz_sup1 = biz.leftOuterJoin(sup)\n",
    "biz_sup1.sortBy(lambda x: x[1][1], ascending=False).take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'94103', (u'10', u'\"1')),\n",
       " (u'94103', (u'6', u'\"1')),\n",
       " (u'94103', (u'8', u'\"1')),\n",
       " (u'94103', (u'3', u'\"1')),\n",
       " (u'94103', (u'9', u'\"1')),\n",
       " (u'94103', (u'5', u'\"1')),\n",
       " (u'94105', (u'6', u'\"1-2-3 Deli')),\n",
       " (u'94105', (u'3', u'\"1-2-3 Deli')),\n",
       " (u'94105', (u'6', u'\"1055 Pine Street')),\n",
       " (u'94105', (u'3', u'\"1055 Pine Street'))]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# only if supervisor exists\n",
    "biz_sup2 = sup.rightOuterJoin(biz)\n",
    "biz_sup2.sortBy(lambda x: x[1][1]).take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "439999"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "biz_sup0.subtract(biz_sup2).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# cogroup\n",
    "biz_zip_cogroup = biz.cogroup(sup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'',\n",
       "  (<pyspark.resultiterable.ResultIterable at 0x10f62c350>,\n",
       "   <pyspark.resultiterable.ResultIterable at 0x110906f50>)),\n",
       " (u'70363',\n",
       "  (<pyspark.resultiterable.ResultIterable at 0x110906f90>,\n",
       "   <pyspark.resultiterable.ResultIterable at 0x110906fd0>)),\n",
       " (u'19443',\n",
       "  (<pyspark.resultiterable.ResultIterable at 0x1109067d0>,\n",
       "   <pyspark.resultiterable.ResultIterable at 0x110906750>)),\n",
       " (u'28036',\n",
       "  (<pyspark.resultiterable.ResultIterable at 0x110906490>,\n",
       "   <pyspark.resultiterable.ResultIterable at 0x1118a7050>)),\n",
       " (u'97302',\n",
       "  (<pyspark.resultiterable.ResultIterable at 0x1118a7090>,\n",
       "   <pyspark.resultiterable.ResultIterable at 0x1118a7150>))]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "biz_zip_cogroup.collect()[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'',\n",
       "  ([u'221 7th Street Residences Llc',\n",
       "    u'Arconas Corporation',\n",
       "    u'Miniclip America Inc',\n",
       "    u'1625 Leavenworth Street Llc',\n",
       "    u'Atc Managed Sites Llc',\n",
       "    u'Lexa Mary C',\n",
       "    u'Red Oxygen Inc',\n",
       "    u'Leo And Janis Paslin Trust',\n",
       "    u'Atc Managed Sites Llc',\n",
       "    u'Hampton Court Sf Lp',\n",
       "    u'Joseph D & Candice M Harney Trust Jc & Cm Harney Trust',\n",
       "    u'Opower Inc',\n",
       "    u'Woo H Woo S',\n",
       "    u'Ultra Electronics Forensic Technology Inc',\n",
       "    u'New Flyer Industriescanada Ulc',\n",
       "    u'Willis Supply Corporation',\n",
       "    u'Endurance Wind Power Inc',\n",
       "    u'Built 1925 Llc',\n",
       "    u'Paybyphone Technologies Inc',\n",
       "    u'Atc Managed Sites Llc',\n",
       "    u'Hartmann Studios Incorporated',\n",
       "    u'Nada Pacific Corporation',\n",
       "    u'Atc Managed Sites Llc',\n",
       "    u'Siemens Public',\n",
       "    u'Iotum Inc',\n",
       "    u'Philz Coffee Inc',\n",
       "    u'Allstream Inc',\n",
       "    u'Vip Plumbing And Drain Cleanin',\n",
       "    u'Bond Blacktop Inc',\n",
       "    u'Htut Chris',\n",
       "    u'Ramon Chavez',\n",
       "    u'Viavid Broadcasting Inc',\n",
       "    u'Cardno Entrix',\n",
       "    u'Vieira Reynaldo',\n",
       "    u'Sara Gulyas',\n",
       "    u'Sfe Energy California Inc',\n",
       "    u'Atc Managed Sites Llc',\n",
       "    u'Law Office Of Scott A Sommer',\n",
       "    u'Atc Managed Sites Llc',\n",
       "    u'Cooper Jim B',\n",
       "    u'Morse Fred',\n",
       "    u'Cast Connex Corporation',\n",
       "    u'Pointclickcare',\n",
       "    u'Radio Ip Software Inc',\n",
       "    u'Cantrell Harris & Assoc Inc',\n",
       "    u'Moonka Nishi',\n",
       "    u'Golden Bay Roofing Inc',\n",
       "    u'Gala Systems Inc',\n",
       "    u'Eldred Robert J Cfp',\n",
       "    u'Atc Managed Sites Llc',\n",
       "    u'Atc Managed Sites Llc',\n",
       "    u'Bibliocommons Inc',\n",
       "    u'Atc Managed Sites Llc',\n",
       "    u'Fts Forest Tech Systems Ltd',\n",
       "    u'Hampton Court Sf Lp',\n",
       "    u'Atc Managed Sites Llc',\n",
       "    u'Intelligent Hospital Systems',\n",
       "    u'Torres Alvaro',\n",
       "    u'Atc Managed Sites Llc',\n",
       "    u'Abd Insurance & Financial Serv',\n",
       "    u'Uniacke J Uniacke M',\n",
       "    u'Atc Managed Sites Llc',\n",
       "    u'Atc Managed Sites Llc',\n",
       "    u'Atc Managed Sites Llc',\n",
       "    u'Ascencion Flores Ismael O',\n",
       "    u'Fairman Mark',\n",
       "    u'C Fischer And Sons Llc',\n",
       "    u'Atc Managed Sites Llc',\n",
       "    u'Magdaluyo Melecio',\n",
       "    u'Ortiz Jose E',\n",
       "    u'Margaret Apartments Lp',\n",
       "    u'Barth Roofing Company Inc',\n",
       "    u'Atc Managed Sites Llc',\n",
       "    u'Intelex Technologies Inc',\n",
       "    u'Atc Managed Sites Llc',\n",
       "    u'Leo And Janis Paslin Trust',\n",
       "    u'East & West Alum Craft Inc',\n",
       "    u'1st Impression Construction Inc',\n",
       "    u'Atc Managed Sites Llc',\n",
       "    u'Atc Managed Sites Llc',\n",
       "    u'Atc Managed Sites Llc',\n",
       "    u'Poco Petroleum Inc',\n",
       "    u'Malik Alia',\n",
       "    u'Rice Paradise Llc',\n",
       "    u'Mid Canada Millwork Ltd',\n",
       "    u'Act Fuels Inc',\n",
       "    u'Atc Managed Sites Llc',\n",
       "    u'Odotech Inc',\n",
       "    u'Delcan Corporation',\n",
       "    u'Cirque Du Soleil Inc',\n",
       "    u'Atc Managed Sites Llc',\n",
       "    u'Atc Managed Sites Llc'],\n",
       "   []))]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "biz_zip_cogroup.map(lambda x: (x[0], (list(x[1][0]), list(x[1][1])))).collect()[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'', (92, 0)),\n",
       " (u'70363', (1, 0)),\n",
       " (u'19443', (1, 0)),\n",
       " (u'28036', (1, 0)),\n",
       " (u'97302', (2, 0))]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "biz_zip_cogroup.map(lambda x : (x[0],(len(list(x[1][0])), len(list(x[1][1]))))).collect()[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 5\n",
    "\n",
    "From ”filtered_registered_business_sf.csv”, create a pair RDD of (zip, (store name, city))\n",
    "+ Count pairs which donot have a key.\n",
    "+ Filter pairs that donot include “San Francisco” in the city value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "biz_raw = sc.textFile('/Users/ThyKhueLy/msan694/data/filtered_registered_business_sf.csv')\n",
    "biz = biz_raw.flatMap(lambda l: l.split(\"\\n\")).map(lambda x: x.split(\",\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[u'94123',\n",
       "  u'Tournahu George L',\n",
       "  u'3301 Broderick St',\n",
       "  u'San Francisco',\n",
       "  u'CA'],\n",
       " [u'94124',\n",
       "  u'Stephens Institute Inc',\n",
       "  u'2225 Jerrold Ave',\n",
       "  u'San Francisco',\n",
       "  u'CA'],\n",
       " [u'94105',\n",
       "  u'Stephens Institute Inc',\n",
       "  u'180 New Montgomery St',\n",
       "  u'San Francisco',\n",
       "  u'CA'],\n",
       " [u'94108',\n",
       "  u'Stephens Institute Inc',\n",
       "  u'540 Powell St',\n",
       "  u'San Francisco',\n",
       "  u'CA'],\n",
       " [u'94107',\n",
       "  u'Stephens Institute Inc',\n",
       "  u'460 Townsend St',\n",
       "  u'San Francisco',\n",
       "  u'CA']]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "biz.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "store_info = biz.map(lambda x: (x[0], (x[1],x[3])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'94123', (u'Tournahu George L', u'San Francisco')),\n",
       " (u'94124', (u'Stephens Institute Inc', u'San Francisco')),\n",
       " (u'94105', (u'Stephens Institute Inc', u'San Francisco')),\n",
       " (u'94108', (u'Stephens Institute Inc', u'San Francisco')),\n",
       " (u'94107', (u'Stephens Institute Inc', u'San Francisco'))]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "store_info.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "92"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# count number which do not have key\n",
    "store_info.countByKey()[u'']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'94110', (u'\"Rlm Partners', u'60 29th St Ste 537')),\n",
       " (u'90805', (u'Lopez Roberto Frias', u'Long+beach')),\n",
       " (u'95492', (u'Denbeste Transportation Inc', u'Windsor')),\n",
       " (u'94510', (u'Trueman Ben', u'Benicia')),\n",
       " (u'94111', (u'Rosewood Venture Associates Iv', u'San+francisco'))]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# store without San Francisco\n",
    "store_info.filter(lambda x: 'San Francisco' not in x[1][1]).distinct().take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tuning Spark – Persist in Memory/Disk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RDDs are by default recomputed each time.\n",
    "However, if you want to reuse an RDD for multiple actions, you can ask Spark to store the content in memory/disk and query repeatedly.\n",
    "```\n",
    "line_with_spark.persist(StorageLevel.persistency_level)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Persistence Level\n",
    "\n",
    "1. **MEMORY_ONLY**\n",
    "Store RDDs in memory, If an RDD does not fit in memory, some partitions will not be cached and will be recomputed on the fly each time they're needed. This is the default level.\n",
    "\n",
    "2. **MEMORY_AND_DISK**\n",
    "If an RDD does not fit in memory, store the partitions that don't fit on disk, and read them from there when they're needed.\n",
    "\n",
    "3. **MEMORY_ONLY_SER (Java and Scala)**\n",
    "Store RDDs as serialized Java objects (one byte array per partition). This is generally more space-efficient than deserialized objects, especially when using a fast serializer, but more CPU-intensive to read.\n",
    " \n",
    "4. **MEMORY_AND_DISK_SER (Java and Scala)**\n",
    "Similar to MEMORY_ONLY_SER, but spill partitions that don't fit in memory to disk instead of recomputing them on the fly each time they're needed.\n",
    "\n",
    "5. **DISK_ONLY**\n",
    "Store RDD partitions only on disk.\n",
    "\n",
    "6. **MEMORY_ONLY_2, MEMORY_AND_DISK_2, etc.**\n",
    "Same as the levels above, but replicate each partition on two cluster nodes.\n",
    "OFF_HEAP (experimental). Similar to MEMORY_ONLY_SER, but store the data in off-heap memory. This requires off-heap memory to be enabled."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are replicated storage options available with each of the storage level.\n",
    "   + Replicated storage levels provide much faster fault recovery than RDD lineage in the event of a task or node failure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `persists(storage_level)`: Many options - memory/disk and replication. \n",
    "    + When using disk options, the persisted data on disk can be used to reconstitute partitions lost due to executor or memory failure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ `.cache() = .persist(MEMORY_ONLY)`: \n",
    "   + Once persisted, RDDs can be reused multiple times without requiring reevaluation (recalculation).\n",
    "   + If there is not enough memory available to cache the RDD, it will be reevaluated for each lineage triggered by an action.\n",
    "   + If we want to change `.persist(MEMORY_ONLY)` --> `.persist(MEMORY_AND_DISK)` will return an error. We will have to call `unpersist()` first\n",
    "\n",
    "\n",
    "+ `.unpersist()` lets you unpersist the RDD."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example without persisting: \n",
    "Input: \n",
    "```python\n",
    "# without persisting\n",
    "start=time.time()\n",
    "lines.count()\n",
    "print \"first \"\n",
    "print time.time() - start\n",
    "\n",
    "start=time.time()\n",
    "lines.count()\n",
    "print \"first \"\n",
    "print time.time() - start\n",
    "```\n",
    "\n",
    "Output: \n",
    "```python \n",
    "first 59.704123\n",
    "second 59.72811\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example with persisting\n",
    "Input: \n",
    "```python\n",
    "# with persisting\n",
    "lines.persist(StorageLevel.MENORY_AND_DISK)\n",
    "start=time.time()\n",
    "lines.count()\n",
    "print \"first \"\n",
    "print time.time() - start # trigger reevalution\n",
    "\n",
    "start=time.time()\n",
    "lines.count()\n",
    "print \"second \"\n",
    "print time.time() - start # does not trigger reevalution\n",
    "\n",
    "start=time.time()\n",
    "lines.count()\n",
    "print \"third \"\n",
    "print time.time() - start # does not trigger reevalution\n",
    "```\n",
    "\n",
    "\n",
    "Output: \n",
    "```python \n",
    "first 79.704123\n",
    "second 20.72811\n",
    "third 20.44669\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check the persistency level\n",
    "getSotrageLevel() – returns different storage option flags set for an RDD.\n",
    "\n",
    "StorageLevel(useDisk, useMemory, useOffHeap, deserialized, replication = 1)\n",
    "+ useDisk : If set, partitions that do not fit in memory will be written to disk.\n",
    "+ useMemory : If set, the RDDs will be stored in-memory.\n",
    "+ useOffHeap : If set, the RDD will be stored outside of the Spark executor in an external system such as Tachyon.\n",
    "+ deserialization : If set, the RDD will be stored as deserialized Java objects.\n",
    "+ replication : An integer that controls the number of copies of the persisted data to be stored."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `MEMORY_ONLY`: Best-Disk only if reevalution is expensive and cannot fit in memory (although sometimes recalculation can be cheaper or fast than reading from the disk)\n",
    "- `OffHeap`: Stored outside of the spark executor in an external system (e.g. Tachyon.) => Use when there is memory issue or noisy cluster\n",
    "- `serialization`: when job is too big to fit in memory\n",
    "- `replication`: faster for recovery, however it will cost more space by making 2 copies and will take sometime to create a second copy. It will be useful when bad connection happens to your cluster. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark’s storage levels are meant to provide different trade-offs between memory usage and CPU efficiency. We recommend going through the following process to select one:\n",
    "\n",
    "If your RDDs fit comfortably with the default storage level (**MEMORY_ONLY**), leave them that way. This is the most CPU-efficient option, allowing operations on the RDDs to run as fast as possible.\n",
    "\n",
    "If not, try using **MEMORY_ONLY_SER** and selecting a fast serialization library to make the objects much more space-efficient, but still reasonably fast to access. (Java and Scala)\n",
    "\n",
    "Don’t spill to disk unless the functions that computed your datasets are expensive, or they filter a large amount of the data. Otherwise, recomputing a partition may be as fast as reading it from disk.\n",
    "\n",
    "Use the replicated storage levels if you want fast fault recovery (e.g. if using Spark to serve requests from a web application). All the storage levels provide full fault tolerance by recomputing lost data, but the replicated ones let you continue running tasks on the RDD without waiting to recompute a lost partition.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 7 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StorageLevel(False, False, False, False, 1)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_name = \"./data/README.md\"\n",
    "lines = sc.textFile(file_name)\n",
    "lines_with_Spark = lines.filter(lambda x: \"Spark\" in x)\n",
    "\n",
    "lines_with_Spark.getStorageLevel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StorageLevel(False, True, False, False, 1)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines_with_Spark.cache()\n",
    "lines_with_Spark.getStorageLevel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines_with_Spark.is_cached"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[95] at RDD at PythonRDD.scala:48"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines_with_Spark.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[95] at RDD at PythonRDD.scala:48"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines_with_Spark.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[95] at RDD at PythonRDD.scala:48"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines_with_Spark.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines_with_Spark.is_cached"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[95] at RDD at PythonRDD.scala:48"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark import StorageLevel\n",
    "lines_with_Spark.unpersist()\n",
    "lines_with_Spark.persist(StorageLevel.DISK_ONLY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[95] at RDD at PythonRDD.scala:48"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines_with_Spark.unpersist()\n",
    "lines_with_Spark.persist(StorageLevel(useDisk=False,\n",
    "                                     useMemory=True,\n",
    "                                     useOffHeap=False,\n",
    "                                     deserialized=False,\n",
    "                                     replication=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes\n",
    "+ If your RDDs fit comfortably with MEMORY_ONLY, leave them that way. \n",
    "    + This is the most CPU-efficient option, allowing operations on the RDDs to run as fast as possible. Iterative algorithms are often good candidates for caching.\n",
    "    \n",
    "    \n",
    "+ Don’t spill to disk unless the functions that computed your datasets are expensive, or they use a large amount of the data. \n",
    "    + Otherwise, recomputing a partition may be as fast as reading it from disk.\n",
    "\n",
    "\n",
    "+ Use the replicated storage levels if you want fast fault recovery (e.g. if using Spark to serve requests from a web application). \n",
    "    + All the storage levels provide full fault tolerance by recomputing lost data, but the replicated ones let you continue running tasks on the RDD without waiting to recompute a lost partition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
